From 436e5c472c4d1e94cc81592c8fa55fede8abd112 Mon Sep 17 00:00:00 2001
From: zwsong <1072860370@qq.com>
Date: Mon, 6 May 2024 21:02:23 +0800
Subject: [PATCH 1/2] =?UTF-8?q?=E7=A7=BB=E6=A4=8Dfastswap?=
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

---
 include/linux/frontswap.h | 21 ++++++++++-
 include/linux/swap.h      |  3 +-
 mm/frontswap.c            | 73 ++++++++++++++++++++++++++++++++++++---
 mm/memcontrol.c           | 50 ++++++++++++++++++---------
 mm/memory.c               |  3 ++
 mm/page_io.c              | 17 ++++++---
 mm/swap_state.c           | 52 +++++++++++++++++-----------
 7 files changed, 171 insertions(+), 48 deletions(-)

diff --git a/include/linux/frontswap.h b/include/linux/frontswap.h
index 011965c08..6a15babc9 100644
--- a/include/linux/frontswap.h
+++ b/include/linux/frontswap.h
@@ -1,4 +1,3 @@
-/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_FRONTSWAP_H
 #define _LINUX_FRONTSWAP_H
 
@@ -11,6 +10,8 @@ struct frontswap_ops {
 	void (*init)(unsigned); /* this swap type was just swapon'ed */
 	int (*store)(unsigned, pgoff_t, struct page *); /* store a page */
 	int (*load)(unsigned, pgoff_t, struct page *); /* load a page */
+	int (*load_async)(unsigned, pgoff_t, struct page *); /* load a page async */
+	int (*poll_load)(int); /* poll cpu for one load */
 	void (*invalidate_page)(unsigned, pgoff_t); /* page no longer needed */
 	void (*invalidate_area)(unsigned); /* swap type just swapoff'ed */
 	struct frontswap_ops *next; /* private pointer to next ops */
@@ -27,6 +28,8 @@ extern bool __frontswap_test(struct swap_info_struct *, pgoff_t);
 extern void __frontswap_init(unsigned type, unsigned long *map);
 extern int __frontswap_store(struct page *page);
 extern int __frontswap_load(struct page *page);
+extern int __frontswap_load_async(struct page *page);
+extern int __frontswap_poll_load(int cpu);
 extern void __frontswap_invalidate_page(unsigned, pgoff_t);
 extern void __frontswap_invalidate_area(unsigned);
 
@@ -93,6 +96,22 @@ static inline int frontswap_load(struct page *page)
 	return -1;
 }
 
+static inline int frontswap_load_async(struct page *page)
+{
+	if (frontswap_enabled())
+		return __frontswap_load_async(page);
+
+	return -1;
+}
+
+static inline int frontswap_poll_load(int cpu)
+{
+	if (frontswap_enabled())
+		return __frontswap_poll_load(cpu);
+
+	return -1;
+}
+
 static inline void frontswap_invalidate_page(unsigned type, pgoff_t offset)
 {
 	if (frontswap_enabled())
diff --git a/include/linux/swap.h b/include/linux/swap.h
index c2b812879..607750833 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -176,7 +176,7 @@ enum {
 	SWP_SCANNING	= (1 << 12),	/* refcount in scan_swap_map */
 };
 
-#define SWAP_CLUSTER_MAX 32UL
+#define SWAP_CLUSTER_MAX 64UL
 #define COMPACT_CLUSTER_MAX SWAP_CLUSTER_MAX
 
 #define SWAP_MAP_MAX	0x3e	/* Max duplication count, in first swap_map */
@@ -388,6 +388,7 @@ extern void kswapd_stop(int nid);
 
 /* linux/mm/page_io.c */
 extern int swap_readpage(struct page *page, bool do_poll);
+extern int swap_readpage_sync(struct page *);
 extern int swap_writepage(struct page *page, struct writeback_control *wbc);
 extern void end_swap_bio_write(struct bio *bio);
 extern int __swap_writepage(struct page *page, struct writeback_control *wbc,
diff --git a/mm/frontswap.c b/mm/frontswap.c
index fec8b5044..dab3321b6 100644
--- a/mm/frontswap.c
+++ b/mm/frontswap.c
@@ -264,8 +264,8 @@ int __frontswap_store(struct page *page)
 	 */
 	if (__frontswap_test(sis, offset)) {
 		__frontswap_clear(sis, offset);
-		for_each_frontswap_ops(ops)
-			ops->invalidate_page(type, offset);
+		// for_each_frontswap_ops(ops)
+		// 	ops->invalidate_page(type, offset);
 	}
 
 	/* Try to store in each implementation, until one succeeds. */
@@ -325,6 +325,49 @@ int __frontswap_load(struct page *page)
 }
 EXPORT_SYMBOL(__frontswap_load);
 
+int __frontswap_load_async(struct page *page)
+{
+	int ret = -1;
+	swp_entry_t entry = { .val = page_private(page), };
+	int type = swp_type(entry);
+	struct swap_info_struct *sis = swap_info[type];
+	pgoff_t offset = swp_offset(entry);
+	struct frontswap_ops *ops;
+
+	VM_BUG_ON(!frontswap_ops);
+	VM_BUG_ON(!PageLocked(page));
+	VM_BUG_ON(sis == NULL);
+
+	if (!__frontswap_test(sis, offset))
+		return -1;
+
+	/* Try loading from each implementation, until one succeeds. */
+	for_each_frontswap_ops(ops) {
+		ret = ops->load_async(type, offset, page);
+		if (!ret) /* successful load */
+			break;
+	}
+	if (ret == 0)
+		inc_frontswap_loads();
+
+	return ret;
+}
+EXPORT_SYMBOL(__frontswap_load_async);
+
+int __frontswap_poll_load(int cpu)
+{
+	struct frontswap_ops *ops;
+
+	VM_BUG_ON(!frontswap_ops);
+
+	/* Try loading from each implementation, until one succeeds. */
+	for_each_frontswap_ops(ops)
+		return ops->poll_load(cpu);
+
+	BUG();
+	return -1;
+}
+EXPORT_SYMBOL(__frontswap_poll_load);
 /*
  * Invalidate any data from frontswap associated with the specified swaptype
  * and offset so that a subsequent "get" will fail.
@@ -332,7 +375,7 @@ EXPORT_SYMBOL(__frontswap_load);
 void __frontswap_invalidate_page(unsigned type, pgoff_t offset)
 {
 	struct swap_info_struct *sis = swap_info[type];
-	struct frontswap_ops *ops;
+	// struct frontswap_ops *ops;
 
 	VM_BUG_ON(!frontswap_ops);
 	VM_BUG_ON(sis == NULL);
@@ -340,8 +383,8 @@ void __frontswap_invalidate_page(unsigned type, pgoff_t offset)
 	if (!__frontswap_test(sis, offset))
 		return;
 
-	for_each_frontswap_ops(ops)
-		ops->invalidate_page(type, offset);
+	// for_each_frontswap_ops(ops)
+		// ops->invalidate_page(type, offset);
 	__frontswap_clear(sis, offset);
 	inc_frontswap_invalidates();
 }
@@ -480,6 +523,25 @@ unsigned long frontswap_curr_pages(void)
 }
 EXPORT_SYMBOL(frontswap_curr_pages);
 
+static int show_curr_pages(struct seq_file *m, void *v)
+{
+	seq_printf(m, "%lu", frontswap_curr_pages());
+	return 0;
+}
+
+static int curr_pages_open(struct inode *inode, struct  file *file)
+{
+	return single_open(file, show_curr_pages, NULL);
+}
+
+static const struct file_operations fops = {
+	.llseek = seq_lseek,
+	.open = curr_pages_open,
+	.owner = THIS_MODULE,
+	.read = seq_read,
+	.release = single_release,
+};
+
 static int __init init_frontswap(void)
 {
 #ifdef CONFIG_DEBUG_FS
@@ -492,6 +554,7 @@ static int __init init_frontswap(void)
 				&frontswap_failed_stores);
 	debugfs_create_u64("invalidates", S_IRUGO,
 				root, &frontswap_invalidates);
+	debugfs_create_file("curr_pages", S_IRUGO, root, NULL, &fops);
 #endif
 	return 0;
 }
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index ac2ffd5e0..decc9f731 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -94,6 +94,8 @@ int do_swap_account __read_mostly;
 #define do_swap_account		0
 #endif
 
+#define FASTSWAP_RECLAIM_CPU	7
+
 /* Whether legacy memory+swap accounting is active */
 static bool do_memsw_account(void)
 {
@@ -1876,12 +1878,23 @@ static void reclaim_high(struct mem_cgroup *memcg,
 	} while ((memcg = parent_mem_cgroup(memcg)));
 }
 
+#define MAX_RECLAIM_OFFLOAD 2048UL
 static void high_work_func(struct work_struct *work)
 {
-	struct mem_cgroup *memcg;
+	struct mem_cgroup *memcg = container_of(work, struct mem_cgroup, high_work);
+	unsigned long high = memcg->high;
+	unsigned long nr_pages = page_counter_read(&memcg->memory);
+	unsigned long reclaim;
+
+	if (nr_pages > high) {
+		reclaim = min(nr_pages - high, MAX_RECLAIM_OFFLOAD);
+
+		/* reclaim_high only reclaims iff nr_pages > high */
+		reclaim_high(memcg, reclaim, GFP_KERNEL);
+	}
 
-	memcg = container_of(work, struct mem_cgroup, high_work);
-	reclaim_high(memcg, CHARGE_BATCH, GFP_KERNEL);
+	if (page_counter_read(&memcg->memory) > memcg->high)
+		schedule_work_on(FASTSWAP_RECLAIM_CPU, &memcg->high_work);
 }
 
 /*
@@ -1912,6 +1925,9 @@ static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,
 	unsigned long nr_reclaimed;
 	bool may_swap = true;
 	bool drained = false;
+	unsigned long high_limit;
+	unsigned long curr_pages;
+	unsigned long excess;
 
 	if (mem_cgroup_is_root(memcg))
 		return 0;
@@ -2040,14 +2056,20 @@ static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,
 	 * reclaim, the cost of mismatch is negligible.
 	 */
 	do {
-		if (page_counter_read(&memcg->memory) > memcg->high) {
-			/* Don't bother a random interrupted task */
-			if (in_interrupt()) {
-				schedule_work(&memcg->high_work);
-				break;
+		high_limit = memcg->high;
+		curr_pages = page_counter_read(&memcg->memory);
+
+		if (curr_pages > high_limit) {
+			excess = curr_pages - high_limit;
+			/* regardless of whether we use app cpu or worker, we evict
+			 * at most MAX_RECLAIM_OFFLOAD pages at a time */
+			if (excess > MAX_RECLAIM_OFFLOAD && !in_interrupt()) {
+				current->memcg_nr_pages_over_high += MAX_RECLAIM_OFFLOAD;
+				set_notify_resume(current);
+			} else {
+				schedule_work_on(FASTSWAP_RECLAIM_CPU, &memcg->high_work);
 			}
-			current->memcg_nr_pages_over_high += batch;
-			set_notify_resume(current);
+
 			break;
 		}
 	} while ((memcg = parent_mem_cgroup(memcg)));
@@ -5186,7 +5208,6 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,
 				 char *buf, size_t nbytes, loff_t off)
 {
 	struct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));
-	unsigned long nr_pages;
 	unsigned long high;
 	int err;
 
@@ -5197,12 +5218,9 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,
 
 	memcg->high = high;
 
-	nr_pages = page_counter_read(&memcg->memory);
-	if (nr_pages > high)
-		try_to_free_mem_cgroup_pages(memcg, nr_pages - high,
-					     GFP_KERNEL, true);
-
+	/* concurrent eviction on shrink */
 	memcg_wb_domain_size_changed(memcg);
+	schedule_work_on(FASTSWAP_RECLAIM_CPU, &memcg->high_work);
 	return nbytes;
 }
 
diff --git a/mm/memory.c b/mm/memory.c
index 793004608..488249916 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -70,6 +70,9 @@
 #include <linux/userfaultfd_k.h>
 #include <linux/dax.h>
 #include <linux/oom.h>
+#include <linux/frontswap.h>
+#include <linux/delay.h>
+
 
 #include <asm/io.h>
 #include <asm/mmu_context.h>
diff --git a/mm/page_io.c b/mm/page_io.c
index e93f1a4ca..9d1b5cb79 100644
--- a/mm/page_io.c
+++ b/mm/page_io.c
@@ -358,11 +358,9 @@ int swap_readpage(struct page *page, bool synchronous)
 	VM_BUG_ON_PAGE(!PageSwapCache(page) && !synchronous, page);
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 	VM_BUG_ON_PAGE(PageUptodate(page), page);
-	if (frontswap_load(page) == 0) {
-		SetPageUptodate(page);
-		unlock_page(page);
+	if (frontswap_load_async(page) == 0)
 		goto out;
-	}
+
 
 	if (sis->flags & SWP_FILE) {
 		struct file *swap_file = sis->swap_file;
@@ -418,6 +416,17 @@ int swap_readpage(struct page *page, bool synchronous)
 	return ret;
 }
 
+int swap_readpage_sync(struct page *page)
+{
+	VM_BUG_ON_PAGE(!PageSwapCache(page), page);
+	VM_BUG_ON_PAGE(!PageLocked(page), page);
+	VM_BUG_ON_PAGE(PageUptodate(page), page);
+
+	BUG_ON(frontswap_load(page));
+
+	return 0;
+}
+
 int swap_set_page_dirty(struct page *page)
 {
 	struct swap_info_struct *sis = page_swap_info(page);
diff --git a/mm/swap_state.c b/mm/swap_state.c
index 39ae7cfad..eaeac7b8a 100644
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@ -21,6 +21,7 @@
 #include <linux/vmalloc.h>
 #include <linux/swap_slots.h>
 #include <linux/huge_mm.h>
+#include <linux/frontswap.h>
 
 #include <asm/pgtable.h>
 
@@ -472,6 +473,19 @@ struct page *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,
 	return retpage;
 }
 
+struct page *read_swap_cache_sync(swp_entry_t entry, gfp_t gfp_mask,
+			struct vm_area_struct *vma, unsigned long addr)
+{
+	bool page_was_allocated;
+	struct page *retpage = __read_swap_cache_async(entry, gfp_mask,
+			vma, addr, &page_was_allocated);
+
+	if (page_was_allocated)
+		swap_readpage_sync(retpage);
+
+	return retpage;
+}
+
 static unsigned int __swapin_nr_pages(unsigned long prev_offset,
 				      unsigned long offset,
 				      int hits,
@@ -554,51 +568,47 @@ static unsigned long swapin_nr_pages(unsigned long offset)
 struct page *swapin_readahead(swp_entry_t entry, gfp_t gfp_mask,
 			struct vm_area_struct *vma, unsigned long addr)
 {
-	struct page *page;
+	struct page *page, *faultpage;
 	unsigned long entry_offset = swp_offset(entry);
 	unsigned long offset = entry_offset;
 	unsigned long start_offset, end_offset;
 	unsigned long mask;
-	struct swap_info_struct *si = swp_swap_info(entry);
-	struct blk_plug plug;
-	bool do_poll = true, page_allocated;
+	int cpu;
+
+	preempt_disable();
+	cpu = smp_processor_id();
+	faultpage = read_swap_cache_sync(entry, gfp_mask, vma, addr);
+	preempt_enable();
 
 	mask = swapin_nr_pages(offset) - 1;
 	if (!mask)
 		goto skip;
 
-	do_poll = false;
 	/* Read a page_cluster sized and aligned cluster around offset. */
 	start_offset = offset & ~mask;
 	end_offset = offset | mask;
 	if (!start_offset)	/* First page is swap header. */
 		start_offset++;
-	if (end_offset >= si->max)
-		end_offset = si->max - 1;
 
-	blk_start_plug(&plug);
 	for (offset = start_offset; offset <= end_offset ; offset++) {
+		if (offset == entry_offset)
+			continue;
+
 		/* Ok, do the async read-ahead now */
-		page = __read_swap_cache_async(
-			swp_entry(swp_type(entry), offset),
-			gfp_mask, vma, addr, &page_allocated);
+		page = read_swap_cache_async(swp_entry(swp_type(entry), offset),
+						gfp_mask, vma, addr, false);//+++read_swap_cache_async(..., bool do_poll)
 		if (!page)
 			continue;
-		if (page_allocated) {
-			swap_readpage(page, false);
-			if (offset != entry_offset &&
-			    likely(!PageTransCompound(page))) {
-				SetPageReadahead(page);
-				count_vm_event(SWAP_RA);
-			}
-		}
+
+		SetPageReadahead(page);
 		put_page(page);
 	}
-	blk_finish_plug(&plug);
 
 	lru_add_drain();	/* Push any new pages onto the LRU now */
+	/* prefetch pages generate interrupts and are handled async */
 skip:
-	return read_swap_cache_async(entry, gfp_mask, vma, addr, do_poll);
+	frontswap_poll_load(cpu);
+	return faultpage;
 }
 
 int init_swap_address_space(unsigned int type, unsigned long nr_pages)
-- 
2.45.1

